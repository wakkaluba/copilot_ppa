# VSCode Local LLM Agent - User Manual

## Table of Contents
1. [Getting Started](#getting-started)
2. [Core Features](#core-features)
3. [Configuration](#configuration)
4. [Usage Scenarios](#usage-scenarios)
5. [Advanced Features](#advanced-features)
6. [Troubleshooting](#troubleshooting)

## Getting Started

### System Requirements
- VS Code 1.60.0+
- Node.js 14.x+
- 8GB RAM minimum
- Local LLM provider (Ollama/LM Studio)

### Initial Setup
1. Install extension
2. Configure LLM provider
3. Set up workspace trust

## Core Features

### Chat Interface
- Access via sidebar
- Command shortcuts
- Context menu integration

### Code Actions
- Code generation
- Refactoring
- Documentation generation
- Code review

### Workspace Integration
- File operations
- Multi-file context
- Project-wide analysis

## Configuration

### LLM Settings
- Provider selection
- Model configuration
- API endpoints
- Cache settings

### Extension Settings
- Auto-start options
- Context depth
- Security levels
- Performance tuning

## Usage Scenarios

### Code Generation
1. Select context
2. Open command palette
3. Choose "Generate Code"
4. Review and apply

### Code Review
1. Select code
2. Right-click > "Review Code"
3. Review suggestions
4. Apply changes

### Documentation
1. Select code/file
2. Command: "Generate Docs"
3. Choose format
4. Review and save

## Advanced Features

### Custom Prompts
- Template creation
- Variable substitution
- Context management

### Batch Operations
- Multi-file processing
- Bulk modifications
- Project-wide changes

### Version Control
- Change preview
- Undo/Redo
- History tracking

## Troubleshooting

### Common Issues
1. Connection Problems
   - Check LLM service
   - Verify endpoints
   - Port conflicts

2. Performance Issues
   - Memory usage
   - Response times
   - Cache management

3. Security Warnings
   - Trust settings
   - Permissions
   - Data handling

### Support
- GitHub Issues
- Documentation
- Community Forums
